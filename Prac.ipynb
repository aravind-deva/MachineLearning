{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "controlled-consultancy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.        , 0.95918367, 0.96078431]),\n",
       " array([1.  , 0.94, 0.98]),\n",
       " array([1.        , 0.97916667, 0.94230769]),\n",
       " 0.9733333333333334,\n",
       " 0.9733333333333334,\n",
       " 0.9733333333333334)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "iris=load_iris()\n",
    "\n",
    "\n",
    "log=LogisticRegression(max_iter=500)\n",
    "log.fit(iris.data, iris.target)\n",
    "classes=log.predict(iris.data)\n",
    "\n",
    "probs=log.predict_proba(iris.data)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "classification_report(iris.target,classes,labels=[0,1])\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "f1_score(iris.target,classes,average=None),recall_score(iris.target,classes,average=None),precision_score(iris.target,classes,average=None),\\\n",
    "f1_score(iris.target,classes,average='micro'),recall_score(iris.target,classes,average='micro'),precision_score(iris.target,classes,average='micro'),\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "departmental-softball",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "ovr=OneVsRestClassifier(LogisticRegression())\n",
    "ovr.fit(iris.data,iris.target)\n",
    "classes=ovr.predict(iris.data)\n",
    "probs=ovr.predict_proba(iris.data)\n",
    "probs.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "former-spirituality",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([9.84064852e-01, 1.13231638e-01, 1.17651820e-06]), 1.0972976662997165)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "bin=LabelBinarizer()\n",
    "bin.fit([0,1,2,])\n",
    "target_2d=bin.transform(iris.target)\n",
    "\n",
    "ovr=OneVsRestClassifier(LogisticRegression())\n",
    "ovr.fit(iris.data,target_2d)\n",
    "classes=ovr.predict(iris.data)\n",
    "probs=ovr.predict_proba(iris.data)\n",
    "probs[0],probs.sum(axis=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ahead-installation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 3) [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1.]\n",
      "(150, 3) [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1.]\n",
      "(150, 3) [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "#from sklearn.datasets import make_multilabel_classification\n",
    "\n",
    "#Just using previous 2d output \n",
    "target_2d[1:4]=[2,2,2]\n",
    "moc=MultiOutputClassifier(LogisticRegression(max_iter=500))\n",
    "moc.fit(iris.data,target_2d)\n",
    "classes=moc.predict(iris.data)\n",
    "probs=moc.predict_proba(iris.data)\n",
    "for prob in probs:\n",
    "    print(prob.shape,prob.sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "concerned-boards",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((105, 4), (105,), (45, 4), (45,))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "a,b,c,d=train_test_split(iris.data,iris.target,test_size=0.3)\n",
    "a.shape,c.shape, b.shape,d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "alien-parallel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    6.90\n",
       "1    3.61\n",
       "2    5.80\n",
       "3    2.20\n",
       "Name: 0.9, dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.percentile(iris.data[:,1],q=95)\n",
    "import pandas as pd\n",
    "df=pd.DataFrame(iris.data)\n",
    "df.quantile(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "sweet-criticism",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1]\n",
      " [0 1 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\advitha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\advitha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.81480247, 0.        , 0.57973867],\n",
       "       [0.        , 0.81480247, 0.57973867]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(['punkt','wordnet'])\n",
    "from nltk.tokenize import word_tokenize\n",
    "words=word_tokenize('sentence to split.full stops   afre covered;nevertheless')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "cv=CountVectorizer( tokenizer=word_tokenize)\n",
    "tokens=cv.fit_transform(['first sentence',' second sentence'])\n",
    "print(tokens.toarray())\n",
    "tfidf=TfidfTransformer()\n",
    "tokens=tfidf.fit_transform(tokens)\n",
    "tokens.toarray()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
